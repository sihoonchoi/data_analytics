{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# Homework 6\n",
    "\n",
    "The block below imports the necessary packages, sets up the \"moons\" dataset you will work with, and sets the random seed for repeatability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pylab as plt\n",
    "\n",
    "from sklearn.datasets import make_moons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(65)\n",
    "\n",
    "X_blob, y_blob = make_moons(n_samples = 300, noise=0.3)\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (5, 4.5), dpi = 150)\n",
    "ax.scatter(X_blob[:, 0], X_blob[:, 1], c = y_blob);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## 1. Separability\n",
    "\n",
    "You've worked on classifying the blob data in the Skill Check. In this Homework, we will work with the \"moons\" dataset and plot the result so that the classification performance can be assessed visually. Choose one loss function among perceptron, softmax, margin loss, counting loss, or suppor vector machine. Use the loss function to perform classification on the dataset above. Create a plot on which the data points are **color-coded by predicted classes** and the resulting **discrimination line is depicted as a black solid line**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "The moons dataset is clearly linearly inseparable. Try using a non-linear transforms of the features to make the data more linearly separable. Plot the new feature vs. one of the original features to check to see how separable it is after the transform.\n",
    "\n",
    "**You are not expected to be able to create a perfectly linearly separable dataset after the transformation. As long as the separability improves you can consider it successful.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "What was your strategy to transform the features? Briefly explain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## 2. Discrimination Line\n",
    "\n",
    "Derive the equation for the line that discriminates between the two classes. Consider a model of the form:\n",
    "\n",
    "$\\bar{\\bar{X}}\\vec{w} > 0$ if $y_i = 1$ (class 1)\n",
    "\n",
    "$\\bar{\\bar{X}}\\vec{w} \\leq 0$ if $y_i = -1$ (class -1)\n",
    "\n",
    "where $\\bar{\\bar{X}}=\\left[\\vec{x_0}, \\vec{x_1}, \\vec{1}\\right]$ and $\\vec{w} = \\left[w_0, w_1, w_2\\right]$.\n",
    "\n",
    "The equation should be in the form of $x_1 = f\\left(x_0\\right)$. Show your work, and/or explain the process you used to arrive at the answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**\n",
    "\n",
    "The discrimination line will be drawn where $\\bar{\\bar{X}}\\vec{w}$=0.\n",
    "  \n",
    "$\\bar{\\bar{X}}\\vec{w}$ = $w_0 x_0$ + $w_1 x_1$ + $w_2$=0\n",
    "  \n",
    "$\\therefore x_1 = -\\frac{w_0}{w_1}x_0 - \\frac{w_2}{w_1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Next, we will consider a model with a non-linear transform defined by:\n",
    "\n",
    "$y_i = w_0x_0 + w_1x_1 + w_2\\left(x^2_0+x^2_1\\right)$\n",
    "\n",
    "where the model predicts class 1 if $y_i > 0$ and predicts class -1 if $y_i \\leq 0$.\n",
    "\n",
    "The equation should in the form of $x_1 = f\\left(x_0\\right)$. Show your work, and/or explain the process you used to arrive at the answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**\n",
    "\n",
    "Discrimination line: $w_0 x_0 + w_1 x_1 + w_2 (x^{2}_{0} + x^{2}_{1})$ = 0\n",
    "\n",
    "$w_2 x^{2}_{1} + w_1 x_1 + w_2 x^{2}_{0} + w_0 x_0$ = 0\n",
    "\n",
    "$x^{2}_{1} + \\frac{w_1}{w_2} x_1 = -x^{2}_{0} - \\frac{w_0}{w_2} x_0$\n",
    "\n",
    "$\\left(x_1 + \\frac{w_1}{2w_2}\\right)^2 = -x_0\\left(x_0 + \\frac{w_0}{w_2}\\right) + \\frac{w^{2}_{1}}{4w^{2}_{2}}$\n",
    "\n",
    "$\\therefore x_1 = \\pm\\sqrt{-x_0\\left(x_0 + \\frac{w_0}{w_2}\\right) + \\frac{w^{2}_{1}}{4w^{2}_{2}}} - \\frac{w_1}{2w_2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Briefly describe the nature of this boundary.\n",
    "\n",
    "What is the shape of the bounday? Is is linear or non-linear? \n",
    "\n",
    "Hint: The form of the equation is a common one that you probably learned in highschool algebra."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**\n",
    "\n",
    "This boundary should be circular and non-linear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## 3. 6745 Only - Select one of the following two problems to solve\n",
    "\n",
    "### 3a: Analytical gradient of the softmax function\n",
    "The gradient of the softmax function is needed during optimization, and having an analytical form makes convergence faster and more numerically stable. \n",
    "\n",
    "Derive an analytical expression for the gradient of the softmax function with respect to $\\vec{w}$.\n",
    "\n",
    "The **softmax** loss function is defined as:\n",
    "\n",
    "$g\\left(\\vec{w}\\right)=\\sum_i \\mathrm{log}\\left(1 + \\mathrm{exp}\\left(-y_i\\vec{x}^T_i\\vec{w}\\right)\\right)$\n",
    "\n",
    "where $\\vec{x}_i$ is the i-th row of the input matrix $\\bar{\\bar{X}}$.\n",
    "\n",
    "***Hint***\n",
    "- The function $g\\left(\\vec{w}\\right)$ can be expressed as $f\\left(r\\left(s\\left(\\vec{w}\\right)\\right)\\right)$ where $r$ and $s$ are arbitrary functions and the chain rule can be applied.\n",
    "- You may want to review Ch.4 of \"Machine Learning Refined, 1st Ed.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**\n",
    "\n",
    "$log\\left\\{1 + exp\\left(-y_i\\vec{x_i}^T\\vec{w}\\right)\\right\\} = f\\left(r\\left(s\\left(\\vec{w}\\right)\\right)\\right)$ where $f(r) = log\\left(r\\right)$, $r(s) = 1 + e^{-s}$, and $s(\\vec{w})=y_i\\vec{x_i}^T\\vec{w}$  \n",
    "\n",
    "Using the chain rule,  \n",
    "\n",
    "$\\frac{\\partial}{\\partial\\vec{w}}f(r(s(\\vec{w})))=\\frac{df}{dr}\\cdot\\frac{dr}{ds}\\cdot\\frac{\\partial}{\\partial\\vec{w}}s(\\vec{w})=\\frac{1}{r}\\cdot(-e^{-s})\\cdot y_i\\vec{x_i}=\\frac{1}{1+e^{-y_i\\vec{x_i}^T\\vec{w}}}\\cdot(-e^{-y_i\\vec{x_i}^T\\vec{w}})\\cdot y_i\\vec{x_i}$  \n",
    "  \n",
    "$\\therefore \\frac{\\partial g(\\vec{w})}{\\partial \\vec{w}}=\\sum_i \\frac{1}{1+e^{-y_i\\vec{x_i}^T\\vec{w}}}\\cdot(-e^{-y_i\\vec{x_i}^T\\vec{w}})\\cdot y_i\\vec{x_i}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "### 3b: Classification as non-linear regression\n",
    "\n",
    "An alternative interpretation of classification is that we are performing non-linear regression to fit a **step function** to our data (because the output is either 0 or 1). Since step functions are not differentiable at the step, a smooth approximation with non-zero derivatives must be used. One such approximation is the ***tanh*** function:\n",
    "\n",
    "$\\mathrm{tanh\\left(x\\right)} = \\frac{2}{1+e^{-x}} - 1$\n",
    "\n",
    "This leads to a reformulation of the classification problem as:\n",
    "\n",
    "$\\vec{y} = \\mathrm{tanh}\\left(\\bar{\\bar{X}}\\vec{w}\\right)$\n",
    "\n",
    "Show that this is mathematically equivalent to **logistic regression**, which is given by minimization of the **softmax** cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**\n",
    "\n",
    "Our expectation in the case of non-linear regression for a step function:\n",
    "\n",
    "$y_i \\approx 1$ if $\\bar{\\bar{X}}\\vec{w} > 0$  \n",
    "\n",
    "$y_i \\approx -1$ if $\\bar{\\bar{X}}\\vec{w} < 0$\n",
    "\n",
    "---\n",
    "\n",
    "In this case, $y_i\\bar{\\bar{X}}\\vec{w}$ will always be positive.\n",
    "\n",
    "$y_i\\bar{\\bar{X}}\\vec{w} > 0 \\rightarrow tanh(y_i\\bar{\\bar{X}}\\vec{w})\\approx1$\n",
    "\n",
    "$tanh(y_i\\bar{\\bar{X}}\\vec{w}) = \\frac{2}{1 + \\exp{(y_i\\bar{\\bar{X}}\\vec{w}})} - 1 \\approx 1$\n",
    "\n",
    "$1 + \\exp{(y_i\\bar{\\bar{X}}\\vec{w}}) \\approx 1$  \n",
    "\n",
    "---\n",
    "\n",
    "Therefore,\n",
    " \n",
    "$log\\left\\{1 + exp(-y_i\\bar{\\bar{X}}\\vec{w})\\right\\}\\approx0$\n",
    "\n",
    "$\\therefore g_{softmax}(\\vec{w}) = \\sum_i log\\left\\{1 + exp(-y_i \\bar{\\bar{X}}\\vec{w})\\right\\}\\approx0$\n",
    "\n",
    "which leads to minimization of the softmax cost function."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 [3.7]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
