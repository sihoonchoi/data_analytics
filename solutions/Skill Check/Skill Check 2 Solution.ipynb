{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# Skill Check 2\n",
    "\n",
    "In this assignment, you will be required to import three external packages (`scikit-learn`, `autograd`, and `scipy`) that were introduced in the lectures and write a few lines of codes with some useful objects in each package. This practice will provide you with an abstract idea of object-oriented programming (OOP). There is no need to know what exactly OOP means, but for the purpose of this course you can think of it as **using programmable \"objects\" made by others to save time**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## 1. scikit-learn (30 pts)\n",
    "\n",
    "`scikit-learn` is the most popular Python package that provides a plethora of useful functions and objects in machine learning. You will go through a workflow of building a simple regression model using `scikit-learn`. You will need to use this skill a lot to build more complicated models for the rest of the semester.\n",
    "\n",
    "Let's import the `scikit-learn` package (no alias needed). (5 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################################\n",
    "# Start your code here\n",
    "import sklearn\n",
    "########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": true,
     "grade_id": "p1-import",
     "locked": true,
     "points": "5",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert sklearn.__version__, \"scikit-learn not imported\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Simple linear regression can be implemented with `scikit-learn`. First, declare a `LinearRegression` model with a variable name `lr`. (5 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# Start your code here\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression()\n",
    "########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "p1-declare",
     "locked": true,
     "points": "5",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert type(lr) == sklearn.linear_model._base.LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "The `LinearRegression` object takes several parameters (or arguments) so that users can easily change the model settings.  You can see the details of parameters as well as the model itself in the [official documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#). In most cases, `scikit-learn` developers have already set the default value for each parameter. For `LinearRegression`, the `fit_intercept` parameter indicates whether the model will add an intercept column to the input matrix during the training process. The default value for `fit_intercept` is `True`.\n",
    "\n",
    "Change `fit_intercept` of `lr` to `False`. (5 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################################\n",
    "# Start your code here\n",
    "lr.fit_intercept = False\n",
    "########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "p1-intercept",
     "locked": true,
     "points": "5",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert lr.fit_intercept == False, \"fit_intercept is True\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Write a function `return_coeff` that takes training data `X` and target values `y` as arguments. In this function, a `LinearRegression` model is trained with `X` and `y` and the resulting coefficients of the model should be returned. Make sure that `fit_intercept` is set to `False`. The data type of the returned variable should be `list`. (15 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def return_coeff(X, y):\n",
    "########################################\n",
    "# Start your code here\n",
    "    lr = LinearRegression(fit_intercept = False)\n",
    "    lr.fit(X, y)\n",
    "    \n",
    "    return list(lr.coef_)\n",
    "########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "p1-coefficients",
     "locked": true,
     "points": "15",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('ethanol_IR.csv')\n",
    "\n",
    "X = df['wavenumber [cm^-1]'].values[500:520].reshape(-1, 1)\n",
    "y = df['absorbance'].values[500:520].reshape(-1, 1)\n",
    "\n",
    "assert np.isclose(np.linalg.norm(return_coeff(X, y)) * np.linalg.norm(return_coeff(y, X)) , 0.6785527874159363), \"return_coeff not correct\"\n",
    "assert type(return_coeff(X, y)) == list, \"the return values should be in list!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## 2. Gradient Descent with autograd (40 pts)\n",
    "\n",
    "In this problem, you will implement gradient descent optimization with functions in the `autograd` package. We will fit IR spectrum peaks with multiple Gaussians to find the optimal positions and widths of the peaks.\n",
    "\n",
    "$$y = \\sum^N_{i=0} w_i exp(-\\frac{(x-\\mu_i)^2}{2\\sigma_i^2})$$\n",
    "\n",
    "First, import `autograd`. (5 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################################\n",
    "# Start your code here\n",
    "import autograd\n",
    "########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": true,
     "grade_id": "p2-import",
     "locked": true,
     "points": "5",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert autograd, \"autograd not imported\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "To implement gradient descent method, you need a well-defined loss function. Create a function `loss` which returns mean-squarred-error of the estimation. `loss` takes the following arguments:\n",
    "\n",
    "- a parameter vector `lamda` $\\vec{\\lambda} = [\\vec{w}, \\vec{\\mu}, \\vec{\\sigma}]$ (1-dimensional numpy array)\n",
    "- a training data `X` (wavenumbers in this case) (1-dimensional numpy array)\n",
    "- a target values `y` (absorbance in this case) (1-dimensional numpy array)\n",
    "- a number of Gaussians `N` (int)\n",
    "\n",
    "You may assume that the length of `lamda` is 3 x `N`. (10 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss(lamda, X, y, N):\n",
    "########################################\n",
    "# Start your code here\n",
    "    predict = np.zeros(X.shape[0])\n",
    "    for i in range(N):\n",
    "        predict += lamda[i] * np.exp(-(X - lamda[i+N])**2 / 2 / lamda[i+2*N]**2)\n",
    "        \n",
    "    return ((predict-y)**2).mean()\n",
    "########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": true,
     "grade_id": "p2-loss",
     "locked": true,
     "points": "10",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('ethanol_IR.csv')\n",
    "\n",
    "X = df['wavenumber [cm^-1]'].values[500:520]\n",
    "y = df['absorbance'].values[500:520]\n",
    "\n",
    "l1 = np.array([5., 5., 5., 3000., 3200., 3300., 50., 50., 50.])\n",
    "ans = loss(l1, X, y, 3)\n",
    "\n",
    "l2 = np.array([10., 11., 12., 13., 14., 2850, 2900., 2950, 3000., 3050., 30., 40., 50., 60., 70.])\n",
    "ans *= loss(l2, X, y, 5)\n",
    "\n",
    "assert np.isclose(ans, 2.2449682077520627), \"loss function not correct\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Using the `grad` function in the `autograd` package, create a function `diff_g` that returns the derivative of the `loss` function with respect to `lamda`. You may assume that `N` equals to 3. (10 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "from autograd import grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################################\n",
    "# Start your code here\n",
    "def g(lamda, X = X, y = y, N = 3):\n",
    "    return loss(lamda, X, y, N)\n",
    "\n",
    "diff_g = grad(g)\n",
    "########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": true,
     "grade_id": "p2-grad",
     "locked": true,
     "points": "10",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "l1 = np.array([5., 5., 5., 3000., 3200., 3300., 50., 50., 50.])\n",
    "assert np.isclose(np.linalg.norm(diff_g(l1)), 0.0034468711), \"diff_g not correct\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, write a function `grad_descent` that implements gradient descent method. This function returns the optimal `lamda` and takes the following arguments:\n",
    "- a parameter vector `lamda` $\\vec{\\lambda}$ (1-dimensional numpy array)\n",
    "- a derivative function `diff_g` (function)\n",
    "- a step size `h` (float)\n",
    "- a tolerance `tol` (float)\n",
    "\n",
    "In numerical optimization, it is very important to set a proper convergence criterion. Optimization should stop once the criterion meets. Various options are available, but, for now, you will compare the the $L_2$ norm of (`current_lamda` - `previous_lamda`) to `tolerance`. If the norm is smaller than `tolerance`, your code should give the `lamda` at that iteration as the optimal solution. (15 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def grad_descent(lamda, diff_g, h, tol):\n",
    "########################################\n",
    "# Start your code here\n",
    "    err = np.inf\n",
    "    previous_lamda = lamda\n",
    "    while err > tol:\n",
    "        current_lamda = previous_lamda - h * np.array(diff_g(previous_lamda))\n",
    "        err = np.linalg.norm(current_lamda - previous_lamda)\n",
    "        previous_lamda = current_lamda\n",
    "        \n",
    "    return current_lamda\n",
    "########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "p2-grad-descent",
     "locked": true,
     "points": "15",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "l1 = np.array([5., 5., 5., 3000., 3200., 3300., 50., 50., 50.])\n",
    "assert np.isclose(np.linalg.norm(grad_descent(l1, diff_g, .1, .001)), 5489.76999348897)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## 3. scipy (30 pts)\n",
    "\n",
    "You will simplify the code that you have written in the previous problem by taking advantage of `scipy` package. `minimize` function in the `scipy` package, which supports numerical optimization and is faster and more reliable. You will find that how convenient and fast it is to code when you find a right function from the internet that fits your interest or intention, although it's always good to have a basic understanding of what is happening \"under the hood\". For more information on the `minimize` function, refer to the [official documentation](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html).\n",
    "\n",
    "Import the `minimize` function from `scipy`. (5 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################################\n",
    "# Start your code here\n",
    "from scipy.optimize import minimize\n",
    "########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": true,
     "grade_id": "p3-import",
     "locked": true,
     "points": "5",
     "solution": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "assert minimize, \"minimize not imported\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "As covered in the lecture, the `minimize` function requires a loss function of which only one argument is unknown. Write a function `g` that takes the same argument as `loss` does, while `X`, `y`, and `N` is predefined and only `lamda` remains unknown. The returned value of `g` should be the same as that of `loss` function. The default values for `X`, `y`, and `N` are provided below. You may wish to review the lecture notes if you are unsure of how to do this. (5 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('ethanol_IR.csv')\n",
    "\n",
    "X = df['wavenumber [cm^-1]'].values[500:520]\n",
    "y = df['absorbance'].values[500:520]\n",
    "\n",
    "N = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################################\n",
    "# Start your code here\n",
    "def g(lamda, X = X, y = y, N = 3):\n",
    "    return loss(lamda, X, y, N)\n",
    "########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": true,
     "grade_id": "p3-g",
     "locked": true,
     "points": "5",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "l = np.array([10., 11., 12., 13., 14., 2850, 2900., 2950, 3000., 3050., 30., 40., 50., 60., 70.])\n",
    "assert np.isclose(g(l, X, y, 5), 290.43601541265457)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Minimize the `g` function with respect to `lamda` by using the scipy `minimize` function. You should use the `L-BFGS-B` algorithm for the optimization.  Save the result to a variable `res`. The initial guess for `lamda` is provided below. The `BFGS` family of algorithms are a good default since they are usually fast and robust. The details of how the algorithms work are beyond this course, but you can read about them [here](https://en.wikipedia.org/wiki/Broyden%E2%80%93Fletcher%E2%80%93Goldfarb%E2%80%93Shanno_algorithm). The short version is that they use clever math to optimize the gradient direction and step size used at each iteration. (20 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "init_lamda = np.array([6., 7., 8., 3500., 3200., 3300., 40., 40., 40.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################################\n",
    "# Start your code here\n",
    "res = minimize(g, init_lamda, method = 'L-BFGS-B')\n",
    "########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": true,
     "grade_id": "p3-minimize",
     "locked": true,
     "points": "20",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert np.isclose(np.linalg.norm(res.x), 5777.971010657634)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 [3.7]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
